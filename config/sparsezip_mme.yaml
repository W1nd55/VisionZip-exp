# SparseZip configuration for MME
# Note: model_type remains 'llava_vzip' â€” SparseZip is the compressor used under the hood.

model:
  model_type: sparsezip
  model_path: liuhaotian/llava-v1.5-7b
  temperature: 0.0
  max_new_tokens: 16

  # Dominant tokens default (SparseZip uses dynamic-K internally; this is a safe numeric default)
  dominant: 54
  # Contextual tokens target (can be overridden by sparsezip.merging.contextual_num)
  contextual: 16

  # SparseZip compressor options (will be used when wiring is enabled)
  sparsezip:
    # Dynamic-K selection: K = round(log(var(scores)+eps) + c)
    dynamic_k: true
    k_min: 8
    k_max: 58  # Optimized: lower than baseline to reduce token count
    dynk:
      c: 7.0  # Optimized: lower average K for better latency
      eps: 1.0e-6

    # Hybrid scoring weights and temps
    alphas:
      attn: 1.0
      entropy: 0.3  # Optimized: enabled for better token selection (conservative)
      mutual: 0.5   # Optimized: enabled for better token selection (conservative)
    tau_feat: 0.2
    tau_sim: 0.1

    # Cross-attention fusion weight (0.0 = off; requires extra wiring in LLM)
    cross_beta: 0.0

    # Contextual merging strategy
    merging:
      contextual_num: 12  # Optimized: slightly lower for better latency
      kmeans_init_factor: 2.0
      kmeans_iters: 10
      agglomerative: true
    
    skip_hybrid_attn: false  # Optimized: enable hybrid scoring
    skip_dynamic_k:   false
    skip_ctx_merge:   false  # Optimized: enable contextual merging   

runner:
  dataset: mme
  # Required at runtime (or pass via --ann_path): path to JSON annotations
  ann_path: null
  output_dir: ./outputs_eval
  warmup: 3
  seed: 42
  limit: null
