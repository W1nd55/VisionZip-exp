# SparseZip configuration for POPE

model:
  model_type: sparsezip
  model_path: liuhaotian/llava-v1.5-7b
  temperature: 0.0
  max_new_tokens: 16

  dominant: 54  # dynamic-K can override internally
  contextual: 12  # Optimized: slightly lower for better latency

  sparsezip:
    dynamic_k: true
    k_min: 32            # INCREASED from 20 (Aggressive accuracy boost)
    k_max: 64            # INCREASED from 48 (Allow more dominant tokens)
    dynk:
      c: 12.0            # INCREASED from 10.0 (Shift K distribution higher)
    k_min: 32            # INCREASED from 20
    k_max: 64            # INCREASED from 48
    dynk:
      c: 12.0            # INCREASED from 10.0
      eps: 1.0e-3

    alphas:
      attn: 1.0
      entropy: 0.8       # INCREASED from 0.6 (Stronger entropy signal)
      mutual: 1.0        # INCREASED from 0.8 (Stronger MI signal)
    tau_feat: 0.15
    tau_sim: 0.08

    # Cross-attention fusion weight (ENABLED for text-aware compression)
    cross_beta: 0.3

    # Contextual merging strategy
    merging:
      contextual_num: 32 # INCREASED from 20 (Keep significantly more context)
      entropy: 0.8       # INCREASED from 0.6
      mutual: 1.0        # INCREASED from 0.8
    tau_feat: 0.15
    tau_sim: 0.08
    cross_beta: 0.1  # ENABLED for text-aware compression

    merging:
      contextual_num: 32 # INCREASED from 20
      kmeans_init_factor: 2.0
      kmeans_iters: 1
      agglomerative: false
    
    skip_hybrid_attn: false
    skip_dynamic_k:   false
    skip_ctx_merge:   false   
    # Feature flags (explicit)
    skip_hybrid_attn: false
    skip_dynamic_k: true
    skip_ctx_merge: false

runner:
  dataset: mme
  ann_path: null
  output_dir: ./outputs_eval
  warmup: 5
  seed: 42
  attn_implementation: "eager"