# SparseZip configuration for MME
# Note: model_type remains 'llava_vzip' â€” SparseZip is the compressor used under the hood.

model:
  model_type: sparsezip
  model_path: liuhaotian/llava-v1.5-7b
  temperature: 0.0
  max_new_tokens: 16

  # Dominant tokens default (SparseZip uses dynamic-K internally; this is a safe numeric default)
  dominant: 54
  # Contextual tokens target (can be overridden by sparsezip.merging.contextual_num)
  contextual: 16

  # SparseZip compressor options (will be used when wiring is enabled)
  sparsezip:
    # Dynamic-K selection: K = round(log(var(scores)+eps) + c)
    dynamic_k: true
    k_min: 32            # INCREASED from 20 (Aggressive accuracy boost)
    k_max: 64            # INCREASED from 48 (Allow more dominant tokens)
    dynk:
      c: 12.0            # INCREASED from 10.0 (Shift K distribution higher)
      eps: 1.0e-3

    # Hybrid scoring weights and temps
    alphas:
      attn: 1.0
      entropy: 0.8       # INCREASED from 0.6 (Stronger entropy signal)
      mutual: 1.0        # INCREASED from 0.8 (Stronger MI signal)
    tau_feat: 0.15
    tau_sim: 0.08

    # Cross-attention fusion weight (0.0 = off; requires extra wiring in LLM)
    cross_beta: 0.0

    # Contextual merging strategy
    merging:
      contextual_num: 32 # INCREASED from 20 (Keep significantly more context)
      kmeans_init_factor: 2.0
      kmeans_iters: 1
      agglomerative: false
    
    skip_hybrid_attn: false
    skip_dynamic_k:   false
    skip_ctx_merge:   false   

runner:
  dataset: mme
  # Required at runtime (or pass via --ann_path): path to JSON annotations
  ann_path: null
  output_dir: ./outputs_eval
  warmup: 3
  seed: 42
  limit: null
