# SparseZip configuration for MME
# Note: model_type remains 'llava_vzip' â€” SparseZip is the compressor used under the hood.

model:
  model_type: sparsezip
  model_path: liuhaotian/llava-v1.5-7b
  temperature: 0.0
  max_new_tokens: 16

  # Dominant tokens default (SparseZip uses dynamic-K internally; this is a safe numeric default)
  dominant: 54
  # Contextual tokens target (can be overridden by sparsezip.merging.contextual_num)
  contextual: 16

  # SparseZip compressor options (will be used when wiring is enabled)
  sparsezip:
    # Dynamic-K selection: K = round(log(var(scores)+eps) + c)
    dynamic_k: true
    k_min: 20            # INCREASED from 8 (less aggressive compression)
    k_max: 48            # DECREASED from 58 (cap dominant tokens lower)
    dynk:
      c: 10.0            # INCREASED from 7.0 (shift K distribution higher)
      eps: 1.0e-3        # INCREASED from 1e-6 (stabilize log)

    # Hybrid scoring weights and temps
    alphas:
      attn: 1.0
      entropy: 0.6       # INCREASED from 0.3 (value entropy more)
      mutual: 0.8        # INCREASED from 0.5 (value MI more)
    tau_feat: 0.15       # DECREASED from 0.2 (sharper entropy)
    tau_sim: 0.08        # DECREASED from 0.1 (sharper MI)

    # Cross-attention fusion weight (0.0 = off; requires extra wiring in LLM)
    cross_beta: 0.0

    # Contextual merging strategy
    merging:
      contextual_num: 20 # INCREASED from 12 (preserve more context)
      kmeans_init_factor: 2.0
      kmeans_iters: 1    # DECREASED from 10 (single-shot now)
      agglomerative: false # DISABLED (not needed with single-shot)
    
    skip_hybrid_attn: false  # Optimized: enable hybrid scoring
    skip_dynamic_k:   false
    skip_ctx_merge:   false  # Optimized: enable contextual merging   

runner:
  dataset: mme
  # Required at runtime (or pass via --ann_path): path to JSON annotations
  ann_path: null
  output_dir: ./outputs_eval
  warmup: 3
  seed: 42
  limit: null
