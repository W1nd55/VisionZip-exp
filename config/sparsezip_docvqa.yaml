# SparseZip configuration for MME
# Note: model_type remains 'llava_vzip' â€” SparseZip is the compressor used under the hood.

model:
  model_type: sparsezip
  model_path: liuhaotian/llava-v1.5-7b
  temperature: 0.0
  max_new_tokens: 16

  # Dominant tokens default (SparseZip uses dynamic-K internally; this is a safe numeric default)
  dominant: 64
  # Contextual tokens target (can be overridden by sparsezip.merging.contextual_num)
  contextual: 16

  # SparseZip compressor options (will be used when wiring is enabled)
  sparsezip:
    # Dynamic-K selection: K = round(log(var(scores)+eps) + c)
    dynamic_k: true
    k_min: 8
    k_max: 64
    dynk:
      c: 8.0
      eps: 1.0e-6

    # Hybrid scoring weights and temps
    alphas:
      attn: 1.0
      entropy: 0
      mutual: 0
    tau_feat: 0.2
    tau_sim: 0.1

    # Cross-attention fusion weight (0.0 = off; requires extra wiring in LLM)
    cross_beta: 0.0

    # Contextual merging strategy
    merging:
      contextual_num: 16
      kmeans_init_factor: 2.0
      kmeans_iters: 10
      agglomerative: true
    
    skip_hybrid_attn: true
    skip_dynamic_k:   true
    skip_ctx_merge:   true   

runner:
  dataset: docvqa   
  # Required at runtime (or pass via --ann_path): path to JSON annotations
  ann_path: null
  output_dir: ./outputs_eval
  warmup: 3
  seed: 42
  limit: null
